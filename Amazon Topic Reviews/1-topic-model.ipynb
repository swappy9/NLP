{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document creation based on Product ID\n",
    "\n",
    "doc1 = ''\n",
    "doc2 = ''\n",
    "doc3 = ''\n",
    "doc4 = ''\n",
    "doc5 = ''\n",
    "doc6 = ''\n",
    "doc7 = ''\n",
    "doc8 = ''\n",
    "doc9 = ''\n",
    "doc10 = ''\n",
    "doc11 = ''\n",
    "doc12 = ''\n",
    "doc13 = ''\n",
    "doc14 = ''\n",
    "doc15 = ''\n",
    "doc16 = ''\n",
    "doc17 = ''\n",
    "doc18 = ''\n",
    "doc19 = ''\n",
    "with open('AmazonData.csv', 'r') as Amazoncsvfile:\n",
    "    csvreader = csv.reader(Amazoncsvfile, delimiter = \"^\")\n",
    "    #for i in productid:\n",
    "    for row in csvreader:\n",
    "        if row[0] == '1':\n",
    "            doc1 += row[-2]\n",
    "        elif row[0] == '2':\n",
    "            doc2 += row[-2]\n",
    "        elif row[0] == '3':\n",
    "            doc3 += row[-2]\n",
    "        elif row[0] == '4':\n",
    "            doc4 += row[-2]\n",
    "        elif row[0] == '5':\n",
    "            doc5 += row[-2]\n",
    "        elif row[0] == '6':\n",
    "            doc6 += row[-2]\n",
    "        elif row[0] == '7':\n",
    "            doc7 += row[-2]\n",
    "        elif row[0] == '8':\n",
    "            doc8 += row[-2]\n",
    "        elif row[0] == '9':\n",
    "            doc9 += row[-2]\n",
    "        elif row[0] == '10':\n",
    "            doc10 += row[-2]\n",
    "        elif row[0] == '11':\n",
    "            doc11 += row[-2]\n",
    "        elif row[0] == '12':\n",
    "            doc12 += row[-2]\n",
    "        elif row[0] == '13':\n",
    "            doc13 += row[-2]\n",
    "        elif row[0] == '14':\n",
    "            doc14 += row[-2]\n",
    "        elif row[0] == '15':\n",
    "            doc15 += row[-2]\n",
    "        elif row[0] == '16':\n",
    "            doc16 += row[-2]\n",
    "        elif row[0] == '17':\n",
    "            doc17 += row[-2]\n",
    "        elif row[0] == '18':\n",
    "            doc18 += row[-2]\n",
    "        elif row[0] == '19':\n",
    "            doc19 += row[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all documents into one based on product ID\n",
    "\n",
    "doc_set = [doc1, doc2, doc3, doc4, doc5, doc6, doc7, doc8, doc9, doc10, \n",
    "           doc11, doc12, doc13, doc14, doc15, doc16, doc17, doc18, doc19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text into lower case\n",
    "\n",
    "doc_lowers = [doc.lower() for doc in doc_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecessary punctuation from the data\n",
    "\n",
    "import string\n",
    "\n",
    "doc_no_punctuation = [doc.translate(str.maketrans('','',string.punctuation))for doc in doc_lowers]\n",
    "#print (doc_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tokesn of text\n",
    "\n",
    "import nltk\n",
    "doc_tokens = [nltk.word_tokenize(doc) for doc in doc_no_punctuation]\n",
    "#print (doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "doc_stemmed = []\n",
    "for one_doc in doc_tokens:\n",
    "    doc_filtered = [w for w in one_doc if not w in stopwords.words(\"english\")]\n",
    "    doc_stem = [ps.stem(w) for w in doc_filtered]\n",
    "    doc_stemmed.append(doc_stem)\n",
    "#print (doc_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Creating dictionary corpus of all the words in the data\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the topic based on each product id\n",
    "\n",
    "corpus=[dictionary.doc2bow(text) for text in doc_stemmed]\n",
    "probability=[[] for x in range(len(corpus))]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes = 20)\n",
    "for i in range(len(corpus)):\n",
    "    probability[i]=ldamodel.get_document_topics(corpus[i],minimum_probability=0)\n",
    "    \n",
    "probability=[[list(p) for p in p1] for p1 in probability]\n",
    "x=ldamodel.show_topics(num_topics=20,num_words=3,formatted=False)\n",
    "topics = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    " \n",
    "for a in probability:\n",
    "    for c,d in zip(a,topics):\n",
    "        c.append(d[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1        0.8725 * ['band', 'great', 'use']          0.1266 * ['great', 'use', 'love']\n",
      "doc 2        0.5939 * ['window', 'room', 'cold']          0.1765 * ['great', 'use', 'love']\n",
      "doc 3        0.562 * ['fire', 'extinguish', 'hous']          0.4348 * ['great', 'use', 'love']\n",
      "doc 4        0.9973 * ['great', 'use', 'love']          0.0001 * ['use', 'great', 'love']\n",
      "doc 5        0.9968 * ['great', 'use', 'love']          0.0002 * ['use', 'great', 'love']\n",
      "doc 6        0.5361 * ['great', 'use', 'love']          0.4617 * ['zest', 'lemon', 'one']\n",
      "doc 7        0.9743 * ['use', 'popcorn', 'make']          0.0014 * ['great', 'use', 'love']\n",
      "doc 8        0.5078 * ['grind', 'coffe', 'grinder']          0.421 * ['use', 'popcorn', 'make']\n",
      "doc 9        0.9432 * ['filter', 'water', 'brita']          0.0335 * ['get', 'line', 'test']\n",
      "doc 10        0.999 * ['use', 'popcorn', 'make']          0.0001 * ['use', 'great', 'love']\n",
      "doc 11        0.9927 * ['use', 'popcorn', 'make']          0.0004 * ['great', 'use', 'love']\n",
      "doc 12        0.989 * ['great', 'use', 'love']          0.0006 * ['use', 'great', 'love']\n",
      "doc 13        0.9951 * ['get', 'line', 'test']          0.0003 * ['great', 'use', 'love']\n",
      "doc 14        0.9985 * ['great', 'use', 'love']          0.0001 * ['use', 'great', 'love']\n",
      "doc 15        0.8474 * ['great', 'use', 'love']          0.1395 * ['grind', 'coffe', 'grinder']\n",
      "doc 16        0.9944 * ['great', 'use', 'love']          0.0003 * ['use', 'great', 'love']\n",
      "doc 17        0.9989 * ['great', 'use', 'love']          0.0001 * ['use', 'great', 'love']\n",
      "doc 18        0.7532 * ['great', 'use', 'love']          0.2428 * ['play', 'nativ', 'kid']\n",
      "doc 19        0.9958 * ['grind', 'coffe', 'grinder']          0.0002 * ['use', 'great', 'love']\n"
     ]
    }
   ],
   "source": [
    "#Printing the topics and it's probability\n",
    "\n",
    "[p.sort(key=lambda x:x[1],reverse=True) for p in probability]\n",
    "for i in range(len(corpus)):\n",
    "    a=probability[i][0:2]\n",
    "    print('doc %s       '%str(i+1), str(round(a[0][1],4)),'*', str(a[0][2]), '        ', str(round(a[1][1],4)),'*',str(a[1][2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
